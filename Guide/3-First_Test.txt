Once you have completed everything under step two, you should be able to run the "takashiTestCompoundRay.py" file since it has the glTF, csv & eye files, and other required dependencies. You may also have to change some of the file directories, but aside from that, the code should work.

Once you have confirmed the display works, you can also make sure it is saving the videos properly by removing the test-videos folder and running it again.

Now, I will explain some important things about how the "takashiTestCompoundRay.py" file works. I don't have the blend file for the Takashi-original-test-scene.gltf, but under blend-file-backups, you can open the basic-manual-scene to look at the different cameras in the scene.

After creating the folder for test-videos, it loads in the eyeRenderer from the "libEyeRenderer3.so" file, which is a library that is updated locally every time you run the cmake and make -j 8 lines from the set-up instructions. If you want to look at how CompoundRay runs or modify its output or functions, you will most likely want to use the "shader.cu" file that can be found within your local compound-ray folder. I will expand more on this file in a different text, but this "shader.cu" file conatains the math for the projection of the image, the assignment of the pixels to the ommatidia colors, and the properties for the various cameras in the blend file that I will expand on later.

It also gives the eyeRenderer functions from eyeTools, which comes from the "eyeRendererHelperFunctions.py" file that is required to have locally. I assume the file's main job is to allow for the use of the eyeRenderer3 library within python.

Next, the glTF is loaded in, the pixel width and height is set, and the result type for each frame is set.

The loop goes through each available camera in the blender file with the "eyeRenderer.nextCamera()" function. I am not 100% sure of this, but I think the reason that it loops through these two cameras specifically, when there are four available in the blend file, is because the regular-panormaic camera is always the first camera, and then the insect-eye-spherical-projector_Or is the only one with the correct .eye file under compound-structure (you can see what I am talking about by opening "basic-manual-scene.blend", selecting object mode in the top left, selecting an orange camera in the scene collection in the top right, and then clicking on the green camera symbol near the bottom right). It's also possible that increasing the for loop beyond 2 will incorporate the other cameras as well.

I will talk more about blender later, but you must always change the .eye file under compound-structure of the insect-eye-spherical-projector_Or, or any other camera if you use a different one, in order to have it work in the python file. The .eye file is also just the .csv file that contains the ommatidia information, just converted by the function convertcsv.py to "convert" the numbers so CompoundRay can use them. I'm not actually sure what this conversion is mathematically, all I know is that it works.

The code then breaks into two seperate parts, if the camera is a compound eye, or if it is not. The panoramic camera is not a compound eye, and I believe the rest of the cameras are. If it is a compound eye, it sets the eye samples per ommatidium, which I believe is how many rays are casted by each ommatidia on to the 3D scene to average and get the color of the ommatidia, so theoretically, higher samples is better for accuracy, although 100 seems to be fine and runs quickly enough.

The next steps display the frame as an rgb image and then add it to the creation of the video with "video.write(bgr)". The display is actually upside down, and so is the final video created because of the -1 in "rgb = eyeRenderer.getFramePointer()[::-1,:,:3]". If you remove this component, the display will remain the same but the final saved video will be flipped up-right. This is a bit strange because the video actually looks correct when it is vertically flipped, meaning the true images produced by CompoundRay are actually visually upside-down. Evidence for this will be shown when you look at the DataExtractionTest, where I actually collect points from the image, although it may be possible that my interpretation is incorrect.

After adding the frame to the video, it loops, moving the camera location with either translateCameraLocally, or rotateCameraLocally. There may be other functions, but these are the only two that I am aware of currently, and should cover most necessary motion. In addition, I have not tested whether or not the movement is accurate to the unit system within the blender file, meaning, if you move forward by 1.00, that may move 1.00m forward in blender, but it may also move by a different amount. This is important information if you want to create scenes and videos that are spatially accurate to the real size of the fly and the environment.

This is everything you need to understand about this first test Takashi provided to me.

